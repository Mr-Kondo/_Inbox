{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4sIw1iH4wP0AyvP2QfvJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "835ffe1a629247fd8c532262f30b97af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56e65b872e4a4d9e9da926fe155a7e21",
              "IPY_MODEL_59766b2260944bf8b11f792308eb0d9f",
              "IPY_MODEL_e5a85d830e3e46fe91869642f0705867"
            ],
            "layout": "IPY_MODEL_4e1bda692116481dbc5e7b17c2b6d758"
          }
        },
        "56e65b872e4a4d9e9da926fe155a7e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a725aa8c1fe43faa66793f3122c4016",
            "placeholder": "​",
            "style": "IPY_MODEL_197285d22026472bbdf6672a5b543511",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "59766b2260944bf8b11f792308eb0d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109ebd4b65f5415b9b54e63062acd80d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d4ec0f359d74501a2b62e1664db44d1",
            "value": 2
          }
        },
        "e5a85d830e3e46fe91869642f0705867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8fe5d8f5bd743cc95da4caf5a6abae6",
            "placeholder": "​",
            "style": "IPY_MODEL_65ef32445f2741e684a902525e645a04",
            "value": " 2/2 [00:23&lt;00:00, 10.91s/it]"
          }
        },
        "4e1bda692116481dbc5e7b17c2b6d758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a725aa8c1fe43faa66793f3122c4016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "197285d22026472bbdf6672a5b543511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109ebd4b65f5415b9b54e63062acd80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d4ec0f359d74501a2b62e1664db44d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8fe5d8f5bd743cc95da4caf5a6abae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ef32445f2741e684a902525e645a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-Kondo/_Inbox/blob/main/local_llm_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ejOYKotHTC_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfedaa7-a518-49be-87fd-dc9762fc2476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF login OK\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = userdata.get(\"HUGGINGFACE_TOKEN\")\n",
        "login(token=token)\n",
        "print(\"HF login OK\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent():\n",
        "    from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "    # シンプルなReActテンプレート\n",
        "    template = \"\"\"You have access to these tools:\n",
        "{tools}\n",
        "\n",
        "Use this EXACT format:\n",
        "Question: [question]\n",
        "Thought: [your reasoning]\n",
        "Action: calculator\n",
        "Action Input: [expression]\n",
        "Observation: [result]\n",
        "Final Answer: [number only]\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    agent = create_react_agent(CHAT, [calculator], prompt)\n",
        "\n",
        "    exec_ = AgentExecutor(\n",
        "        agent=agent, tools=[calculator],\n",
        "        return_intermediate_steps=True,\n",
        "        handle_parsing_errors=True,\n",
        "        max_iterations=5,  # 30→5に削減（無限ループ防止）\n",
        "        max_execution_time=20,  # 45→20秒に短縮\n",
        "        early_stopping_method=\"force\",\n",
        "        verbose=True  # デバッグ用\n",
        "    )\n",
        "    return exec_\n"
      ],
      "metadata": {
        "id": "S8umM2Zwesyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルが基本的な指示に従えるかテスト\n",
        "test_prompts = [\n",
        "    \"Output only the number 42:\",\n",
        "    \"What is 2+2? Answer with only a number:\",\n",
        "    '{\"ok\": true}',\n",
        "]\n",
        "\n",
        "print(\"\\n=== BASIC MODEL TEST ===\")\n",
        "for p in test_prompts:\n",
        "    messages = [{\"role\": \"user\", \"content\": p}]\n",
        "    formatted = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    result = gen_pipe(formatted, max_new_tokens=50, return_full_text=False)\n",
        "    output = result[0][\"generated_text\"]\n",
        "    print(f\"Prompt: {p}\")\n",
        "    print(f\"Output: {output}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFgv-cS-exSx",
        "outputId": "88eb2bec-392a-4e52-8970-32b5432d0729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BASIC MODEL TEST ===\n",
            "Prompt: Output only the number 42:\n",
            "Output: 42\n",
            "------------------------------------------------------------\n",
            "Prompt: What is 2+2? Answer with only a number:\n",
            "Output: 4\n",
            "------------------------------------------------------------\n",
            "Prompt: {\"ok\": true}\n",
            "Output: It seems like you're providing a JSON object with a single key-value pair. However, it doesn't appear to be a complete or meaningful object. Can you please provide more context or clarify what you're trying to accomplish? I'll do my best\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== (0) 依存（必要に応じて実行） =========================================\n",
        "!pip -q install -U transformers accelerate bitsandbytes sentencepiece huggingface_hub\n",
        "!pip -q install -U langchain langchain-huggingface langgraph\n",
        "\n",
        "\n",
        "# ==== (1) 共通セットアップ ====================================================\n",
        "import os\n",
        "import warnings\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "# LangSmith完全無効化\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='langsmith')\n",
        "\n",
        "import csv, json, time, re, datetime, pathlib\n",
        "import torch\n",
        "from time import perf_counter\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# ---- HFトークン取得 ---------------------------------------------------------\n",
        "HF_TOKEN = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "    print(\"✓ HF token loaded from Colab Secrets\")\n",
        "except Exception:\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        HF_TOKEN = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "        print(\"✓ HF token loaded from Kaggle Secrets\")\n",
        "    except Exception:\n",
        "        HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "        if HF_TOKEN:\n",
        "            print(\"✓ HF token loaded from environment\")\n",
        "\n",
        "# ---- HF 4bitロード（NF4） ---------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, token=HF_TOKEN)\n",
        "mdl = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_cfg,\n",
        "    torch_dtype=torch.float16,  # ✅ 修正：torch_dtype → dtype警告を回避\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Llama系はpad未定義→pad=eosを明示\n",
        "tok.pad_token_id = tok.eos_token_id\n",
        "mdl.generation_config.pad_token_id = tok.pad_token_id\n",
        "\n",
        "# ✅ 修正：GEN_KWをパイプライン作成「前」に定義\n",
        "GEN_KW = dict(\n",
        "    max_new_tokens=256,  # ✅ 128→256\n",
        "    do_sample=False,\n",
        "    pad_token_id=tok.pad_token_id,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "# 警告抑制\n",
        "import logging\n",
        "logging.getLogger(\"transformers.pipelines\").setLevel(logging.ERROR)\n",
        "\n",
        "# 生成パイプライン\n",
        "gen_pipe = pipeline(\"text-generation\", model=mdl, tokenizer=tok, **GEN_KW)\n",
        "\n",
        "# メモリ確認\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "print(f\"GPU reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "# ---- LangChainラッパ ---------------------------------------------------------\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# ChatHuggingFaceを使わず、HuggingFacePipelineを直接使用\n",
        "LLM = HuggingFacePipeline(pipeline=gen_pipe)\n",
        "\n",
        "# チャット形式を手動で適用するヘルパー関数\n",
        "def format_for_llama(prompt: str) -> str:\n",
        "    \"\"\"Llama-3.2のチャット形式に変換\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "print(\"✓ Model and pipeline ready\\n\")\n",
        "\n",
        "\n",
        "# ==== (2) ツール／Runner実装 ==================================================\n",
        "\n",
        "# ✅ safe_calc関数を最初に定義\n",
        "def safe_calc(expr: str) -> str:\n",
        "    \"\"\"Evaluate arithmetic expression safely.\"\"\"\n",
        "    expr = expr.replace(\"^\", \"**\")  # べき乗演算子を変換\n",
        "    # 安全な文字のみ許可\n",
        "    if not re.fullmatch(r\"[0-9\\+\\-\\*/\\(\\)\\s\\*]+\", expr):\n",
        "        return \"ERROR: invalid characters\"\n",
        "    try:\n",
        "        # 安全な環境でeval実行\n",
        "        val = eval(expr, {\"__builtins__\": {}}, {})\n",
        "        return str(int(val))\n",
        "    except Exception as e:\n",
        "        return f\"ERROR: {e}\"\n",
        "\n",
        "# --- Chain (LCEL) -------------------------------------------------------------\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def run_chain(prompt: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        print(f\"[CHAIN] Starting...\", flush=True)\n",
        "\n",
        "        # 直接フォーマットして実行\n",
        "        formatted = format_for_llama(prompt)\n",
        "\n",
        "        t0 = perf_counter()\n",
        "        result = gen_pipe(formatted)\n",
        "        out = result[0][\"generated_text\"].strip()\n",
        "        t1 = perf_counter()\n",
        "\n",
        "        print(f\"[CHAIN] Output: {out[:80]}\", flush=True)\n",
        "        return {\"output\": out, \"ms\": int((t1 - t0) * 1000), \"tool_calls\": 0}\n",
        "    except Exception as e:\n",
        "        print(f\"[CHAIN ERROR] {e}\", flush=True)\n",
        "        traceback.print_exc()\n",
        "        return {\"output\": f\"ERROR: {e}\", \"ms\": 0, \"tool_calls\": 0}\n",
        "\n",
        "# --- Agent (ReAct) ------------------------------------------------------------\n",
        "from langchain import hub\n",
        "try:\n",
        "    from langchain.tools import tool\n",
        "except Exception:\n",
        "    from langchain_core.tools import tool\n",
        "\n",
        "@tool(\"calculator\")\n",
        "def calculator(expr: str) -> str:\n",
        "    \"\"\"Evaluate arithmetic like '231*47 + 5^3' and return the integer result.\"\"\"\n",
        "    return safe_calc(expr)\n",
        "\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "\n",
        "@tool(\"calculator\")\n",
        "def calculator(expr: str) -> str:\n",
        "    \"\"\"Evaluate arithmetic like '231*47 + 5^3' and return the integer result.\"\"\"\n",
        "    return safe_calc(expr)\n",
        "\n",
        "def run_agent(prompt: str) -> Dict[str, Any]:\n",
        "    \"\"\"簡易Agent：ツールが必要そうなら呼び出す、そうでなければ直接回答\"\"\"\n",
        "    try:\n",
        "        print(f\"[AGENT] Starting...\", flush=True)\n",
        "        t0 = perf_counter()\n",
        "        tool_calls = 0\n",
        "\n",
        "        # ステップ1: LLMに判断させる\n",
        "        decision_prompt = f\"\"\"Task: {prompt}\n",
        "\n",
        "If this needs calculation, respond ONLY:\n",
        "USE_CALC: expression\n",
        "\n",
        "Otherwise respond ONLY:\n",
        "ANSWER: your answer\"\"\"\n",
        "\n",
        "        formatted = format_for_llama(decision_prompt)\n",
        "        result = gen_pipe(formatted, max_new_tokens=64)\n",
        "        response = result[0][\"generated_text\"].strip()\n",
        "\n",
        "        print(f\"[AGENT] LLM response: {response[:100]}\", flush=True)\n",
        "\n",
        "        # ステップ2: ツール呼び出しの判定\n",
        "        if \"USE_CALC:\" in response:\n",
        "            # 式を抽出\n",
        "            expr = response.split(\"USE_CALC:\", 1)[1].strip().split()[0]\n",
        "            print(f\"[AGENT] Calling calculator: {expr}\", flush=True)\n",
        "            calc_result = calculator.invoke(expr)\n",
        "            tool_calls = 1\n",
        "\n",
        "            # ステップ3: 結果を整形\n",
        "            final_prompt = f\"The calculation result is {calc_result}. Output only this number:\"\n",
        "            formatted2 = format_for_llama(final_prompt)\n",
        "            result2 = gen_pipe(formatted2, max_new_tokens=32)\n",
        "            final_answer = result2[0][\"generated_text\"].strip()\n",
        "        else:\n",
        "            # ANSWER:から抽出\n",
        "            if \"ANSWER:\" in response:\n",
        "                final_answer = response.split(\"ANSWER:\", 1)[1].strip()\n",
        "            else:\n",
        "                final_answer = response\n",
        "\n",
        "        t1 = perf_counter()\n",
        "        print(f\"[AGENT] Final: {final_answer[:80]}\", flush=True)\n",
        "\n",
        "        return {\n",
        "            \"output\": final_answer,\n",
        "            \"ms\": int((t1 - t0) * 1000),\n",
        "            \"tool_calls\": tool_calls,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"[AGENT ERROR] {e}\", flush=True)\n",
        "        traceback.print_exc()\n",
        "        return {\"output\": f\"ERROR: {e}\", \"ms\": 0, \"tool_calls\": 0}\n",
        "\n",
        "\n",
        "# --- LangGraph（修正版） ---------------------------------------------------\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.errors import GraphRecursionError\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "GRAPH_INSTR = \"\"\"You can use the 'calculator' tool.\n",
        "\n",
        "If calculation needed, respond ONLY:\n",
        "{\"action\":\"calculator\",\"action_input\":\"231*47+5^3\"}\n",
        "\n",
        "Otherwise respond:\n",
        "Final Answer: [your answer]\"\"\"\n",
        "\n",
        "def _ai_says_action(txt: str):\n",
        "    try:\n",
        "        start = txt.index(\"{\")\n",
        "        end = txt.rindex(\"}\") + 1\n",
        "        js = json.loads(txt[start:end])\n",
        "        if isinstance(js, dict) and js.get(\"action\") == \"calculator\":\n",
        "            return js.get(\"action_input\", \"\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def build_graph():\n",
        "    try:\n",
        "        print(\"[GRAPH] Building...\", flush=True)\n",
        "        g = StateGraph(dict)\n",
        "\n",
        "        def llm_node(state: dict) -> dict:\n",
        "            # メッセージからテキスト抽出してフォーマット\n",
        "            last_msg = state[\"messages\"][-1]\n",
        "            if isinstance(last_msg, HumanMessage):\n",
        "                prompt = last_msg.content\n",
        "            else:\n",
        "                prompt = str(last_msg)\n",
        "\n",
        "            formatted = format_for_llama(prompt)\n",
        "            result = gen_pipe(formatted)\n",
        "            resp_text = result[0][\"generated_text\"].strip()\n",
        "\n",
        "            state[\"messages\"].append(AIMessage(content=resp_text))\n",
        "            return state\n",
        "\n",
        "        def route(state: dict):\n",
        "            last = state[\"messages\"][-1]\n",
        "            if isinstance(last, AIMessage):\n",
        "                txt = (last.content or \"\").strip()\n",
        "                act = _ai_says_action(txt)\n",
        "                if act is not None:\n",
        "                    return \"tools\"\n",
        "                if \"Final Answer:\" in txt:\n",
        "                    return END\n",
        "            return \"llm\"\n",
        "\n",
        "        def tools_node(state: dict) -> dict:\n",
        "            last = state[\"messages\"][-1].content\n",
        "            expr = _ai_says_action(last) or \"\"\n",
        "            result = calculator.invoke(expr)\n",
        "            obs = AIMessage(content=f\"Observation: {result}\\nFinal Answer: {result}\")\n",
        "            state[\"messages\"].append(obs)\n",
        "            state[\"tool_calls\"] = state.get(\"tool_calls\", 0) + 1\n",
        "            return state\n",
        "\n",
        "        g.add_node(\"llm\", llm_node)\n",
        "        g.add_node(\"tools\", tools_node)\n",
        "        g.add_conditional_edges(\"llm\", route, {\"llm\": \"llm\", \"tools\": \"tools\", END: END})\n",
        "        g.add_edge(\"tools\", END)\n",
        "        g.set_entry_point(\"llm\")\n",
        "\n",
        "        compiled = g.compile()\n",
        "        print(\"[GRAPH] ✓ Graph ready\", flush=True)\n",
        "        return compiled\n",
        "    except Exception as e:\n",
        "        print(f\"[GRAPH BUILD ERROR] {e}\", flush=True)\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "GRAPH_APP = build_graph()\n",
        "\n",
        "def run_graph(prompt: str, rec_limit: int = 25) -> Dict[str, Any]:\n",
        "    try:\n",
        "        print(f\"[GRAPH] Starting...\", flush=True)\n",
        "        t0 = perf_counter()\n",
        "        state = {\"messages\": [HumanMessage(content=f\"{GRAPH_INSTR}\\n\\nQuestion: {prompt}\")]}\n",
        "        out_state = GRAPH_APP.invoke(state, config={\"recursion_limit\": rec_limit})\n",
        "        t1 = perf_counter()\n",
        "\n",
        "        final = \"\"\n",
        "        for m in reversed(out_state[\"messages\"]):\n",
        "            if isinstance(m, AIMessage):\n",
        "                final = m.content or \"\"\n",
        "                break\n",
        "\n",
        "        tools_used = out_state.get(\"tool_calls\", 0)\n",
        "        print(f\"[GRAPH] Completed, tools={tools_used}\", flush=True)\n",
        "        return {\"output\": final, \"ms\": int((t1 - t0) * 1000), \"tool_calls\": tools_used}\n",
        "\n",
        "    except GraphRecursionError:\n",
        "        print(f\"[GRAPH] Recursion limit\", flush=True)\n",
        "        return {\"output\": \"ERROR: recursion limit\", \"ms\": int((perf_counter()-t0)*1000), \"tool_calls\": 0}\n",
        "    except Exception as e:\n",
        "        print(f\"[GRAPH ERROR] {e}\", flush=True)\n",
        "        traceback.print_exc()\n",
        "        return {\"output\": f\"ERROR: {e}\", \"ms\": 0, \"tool_calls\": 0}\n",
        "\n",
        "\n",
        "## ==== (3) タスク定義＆評価 =====================================================\n",
        "\n",
        "# ✅ 評価関数を先に定義\n",
        "def eval_json_ok(s: str) -> bool:\n",
        "    \"\"\"JSONが {\"ok\": true} であるかチェック\"\"\"\n",
        "    try:\n",
        "        # 余計なテキストを除去してJSON部分を抽出\n",
        "        s = s.strip()\n",
        "        # {\"ok\": true} だけの場合\n",
        "        if s == '{\"ok\": true}' or s == \"{'ok': True}\":\n",
        "            return True\n",
        "        # JSONパース試行\n",
        "        obj = json.loads(s)\n",
        "        return isinstance(obj, dict) and obj.get(\"ok\") is True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def eval_eq_int(s: str, expect: int) -> bool:\n",
        "    \"\"\"出力が期待する整数と一致するかチェック\"\"\"\n",
        "    s = s.strip()\n",
        "\n",
        "    # ✅ \"Observation: 数字\" パターンを追加\n",
        "    if \"Observation:\" in s:\n",
        "        # \"Observation: 10982 Final Answer: 10982\" から数字を抽出\n",
        "        obs_match = re.search(r'Observation:\\s*(\\d+)', s)\n",
        "        if obs_match:\n",
        "            try:\n",
        "                return int(obs_match.group(1)) == expect\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # 既存のロジック\n",
        "    if s.startswith(\"Final Answer:\"):\n",
        "        s = s.split(\"Final Answer:\", 1)[1].strip()\n",
        "\n",
        "    try:\n",
        "        num_str = re.sub(r\"[^\\d\\-]\", \"\", s)\n",
        "        if not num_str:\n",
        "            return False\n",
        "        return int(num_str) == expect\n",
        "    except Exception:\n",
        "        return False\n",
        "        return False\n",
        "\n",
        "def eval_contains_one(s: str, options: List[str]) -> bool:\n",
        "    \"\"\"出力がoptions内の1つの単語と完全一致するかチェック\"\"\"\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"Final Answer:\"):\n",
        "        s = s.split(\":\", 1)[1].strip()\n",
        "    # 余計な句読点を除去\n",
        "    s = re.sub(r'[。、\\s]+', '', s)\n",
        "    return s in options\n",
        "\n",
        "def eval_email(s: str, expect: str) -> bool:\n",
        "    \"\"\"メールアドレスを抽出してチェック\"\"\"\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"Final Answer:\"):\n",
        "        s = s.split(\":\", 1)[1].strip()\n",
        "    # メールアドレスパターンを抽出\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    matches = re.findall(email_pattern, s)\n",
        "    if matches:\n",
        "        return matches[0].lower() == expect.lower()\n",
        "    # 直接一致もチェック\n",
        "    return s.lower() == expect.lower()\n",
        "\n",
        "# ✅ タスク定義（評価関数の後に配置）\n",
        "ALL_TASKS = {\n",
        "    \"s0_format\": {\n",
        "        \"prompt\": 'Output EXACTLY: {\"ok\": true}',\n",
        "        \"eval\": lambda s: eval_json_ok(s),\n",
        "    },\n",
        "    \"s1_reasoning\": {\n",
        "        \"prompt\": \"What is 40+2? Output only the number:\",\n",
        "        \"eval\": lambda s: eval_eq_int(s, 42),\n",
        "    },\n",
        "    \"s3_tool\": {\n",
        "        \"prompt\": \"Calculate: 231*47 + 5^3. Output only the final integer:\",\n",
        "        \"eval\": lambda s: eval_eq_int(s, 10982),\n",
        "    },\n",
        "    \"s4_context_qa\": {\n",
        "        \"prompt\": \"Context: 日本の古都として有名なのは京都と奈良です。Extract ONE city name. Output only one word:\",\n",
        "        \"eval\": lambda s: eval_contains_one(s, [\"京都\", \"奈良\"]),\n",
        "    },\n",
        "    \"s5_extract_email\": {\n",
        "        \"prompt\": \"Text: 連絡先は info@example.com です。Extract the email address:\",\n",
        "        \"eval\": lambda s: eval_email(s, \"info@example.com\"),\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "#==== (4) 実行ループ（すべてのタスク×3方式） ================================\n",
        "RUN_TASKS = list(ALL_TASKS.keys())  # 実行したいタスクを絞る場合は編集\n",
        "RUNNERS = {\n",
        "    \"chain\": run_chain,\n",
        "    \"agent\": run_agent,\n",
        "    \"graph\": run_graph,\n",
        "}\n",
        "\n",
        "stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_dir = pathlib.Path(\"results_\"+stamp); out_dir.mkdir(exist_ok=True)\n",
        "csv_path = out_dir / \"results.csv\"\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\n",
        "        \"trial\",\"runner\",\"task\",\"wall_time_ms\",\"tool_calls\",\"passed\",\"output\"\n",
        "    ])\n",
        "    writer.writeheader()\n",
        "    TRIALS = 3  # 回数は適宜変更\n",
        "    for t in range(1, TRIALS+1):\n",
        "        for task_name in RUN_TASKS:\n",
        "            P = ALL_TASKS[task_name][\"prompt\"]\n",
        "            E = ALL_TASKS[task_name][\"eval\"]\n",
        "            for rname, fn in RUNNERS.items():\n",
        "                res = fn(P)\n",
        "                passed = bool(E(res[\"output\"]))\n",
        "                writer.writerow({\n",
        "                    \"trial\": t, \"runner\": rname, \"task\": task_name,\n",
        "                    \"wall_time_ms\": res[\"ms\"], \"tool_calls\": res[\"tool_calls\"],\n",
        "                    \"passed\": int(passed), \"output\": res[\"output\"][:200].replace(\"\\n\",\" \"),\n",
        "                })\n",
        "                print(f\"[t{t}] {rname:<5} | {task_name:<14} | {res['ms']:>5} ms | tools={res['tool_calls']} | pass={passed}\")\n",
        "\n",
        "print(f\"\\nSaved -> {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "835ffe1a629247fd8c532262f30b97af",
            "56e65b872e4a4d9e9da926fe155a7e21",
            "59766b2260944bf8b11f792308eb0d9f",
            "e5a85d830e3e46fe91869642f0705867",
            "4e1bda692116481dbc5e7b17c2b6d758",
            "0a725aa8c1fe43faa66793f3122c4016",
            "197285d22026472bbdf6672a5b543511",
            "109ebd4b65f5415b9b54e63062acd80d",
            "1d4ec0f359d74501a2b62e1664db44d1",
            "e8fe5d8f5bd743cc95da4caf5a6abae6",
            "65ef32445f2741e684a902525e645a04"
          ]
        },
        "id": "kTGYOPtmfaen",
        "outputId": "d3008508-ed66-4637-fe0f-6ab8a9df3780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ HF token loaded from Colab Secrets\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "835ffe1a629247fd8c532262f30b97af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU allocated: 4.75 GB\n",
            "GPU reserved: 6.08 GB\n",
            "✓ Model and pipeline ready\n",
            "\n",
            "[GRAPH] Building...\n",
            "[GRAPH] ✓ Graph ready\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: {\"ok\": true}\n",
            "[t1] chain | s0_format      |  1671 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: {\"ok\": true}\n",
            "[AGENT] Final: {\"ok\": true}\n",
            "[t1] agent | s0_format      |   742 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t1] graph | s0_format      | 355519 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 42\n",
            "[t1] chain | s1_reasoning   |   147 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 42\n",
            "[AGENT] Final: 42\n",
            "[t1] agent | s1_reasoning   |   382 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t1] graph | s1_reasoning   |   387 ms | tools=0 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: To calculate the expression, we need to follow the order of operations (PEMDAS):\n",
            "[t1] chain | s3_tool        |  4488 ms | tools=0 | pass=False\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 10947\n",
            "[AGENT] Final: 10947\n",
            "[t1] agent | s3_tool        |   430 ms | tools=0 | pass=False\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=1\n",
            "[t1] graph | s3_tool        |  1255 ms | tools=1 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 京都\n",
            "[t1] chain | s4_context_qa  |   211 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: 京都\n",
            "[AGENT] Final: 京都\n",
            "[t1] agent | s4_context_qa  |   215 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t1] graph | s4_context_qa  | 354217 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: The email address is: info@example.com\n",
            "[t1] chain | s5_extract_email |   528 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: info@example.com\n",
            "[AGENT] Final: info@example.com\n",
            "[t1] agent | s5_extract_email |   435 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t1] graph | s5_extract_email |   418 ms | tools=0 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: {\"ok\": true}\n",
            "[t2] chain | s0_format      |   375 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: {\"ok\": true}\n",
            "[AGENT] Final: {\"ok\": true}\n",
            "[t2] agent | s0_format      |   542 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t2] graph | s0_format      | 354171 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 42\n",
            "[t2] chain | s1_reasoning   |   214 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 42\n",
            "[AGENT] Final: 42\n",
            "[t2] agent | s1_reasoning   |   488 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t2] graph | s1_reasoning   |   365 ms | tools=0 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: To calculate the expression, we need to follow the order of operations (PEMDAS):\n",
            "[t2] chain | s3_tool        |  4496 ms | tools=0 | pass=False\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 10947\n",
            "[AGENT] Final: 10947\n",
            "[t2] agent | s3_tool        |   417 ms | tools=0 | pass=False\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=1\n",
            "[t2] graph | s3_tool        |   961 ms | tools=1 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 京都\n",
            "[t2] chain | s4_context_qa  |   141 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: 京都\n",
            "[AGENT] Final: 京都\n",
            "[t2] agent | s4_context_qa  |   143 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t2] graph | s4_context_qa  | 354053 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: The email address is: info@example.com\n",
            "[t2] chain | s5_extract_email |   703 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: info@example.com\n",
            "[AGENT] Final: info@example.com\n",
            "[t2] agent | s5_extract_email |   502 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t2] graph | s5_extract_email |   412 ms | tools=0 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: {\"ok\": true}\n",
            "[t3] chain | s0_format      |   366 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: {\"ok\": true}\n",
            "[AGENT] Final: {\"ok\": true}\n",
            "[t3] agent | s0_format      |   510 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t3] graph | s0_format      | 354547 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 42\n",
            "[t3] chain | s1_reasoning   |   155 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 42\n",
            "[AGENT] Final: 42\n",
            "[t3] agent | s1_reasoning   |   355 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t3] graph | s1_reasoning   |   362 ms | tools=0 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: To calculate the expression, we need to follow the order of operations (PEMDAS):\n",
            "[t3] chain | s3_tool        |  5083 ms | tools=0 | pass=False\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: 10947\n",
            "[AGENT] Final: 10947\n",
            "[t3] agent | s3_tool        |   420 ms | tools=0 | pass=False\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=1\n",
            "[t3] graph | s3_tool        |   962 ms | tools=1 | pass=True\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: 京都\n",
            "[t3] chain | s4_context_qa  |   143 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: 京都\n",
            "[AGENT] Final: 京都\n",
            "[t3] agent | s4_context_qa  |   145 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Recursion limit\n",
            "[t3] graph | s4_context_qa  | 353973 ms | tools=0 | pass=False\n",
            "[CHAIN] Starting...\n",
            "[CHAIN] Output: The email address is: info@example.com\n",
            "[t3] chain | s5_extract_email |   537 ms | tools=0 | pass=True\n",
            "[AGENT] Starting...\n",
            "[AGENT] LLM response: ANSWER: info@example.com\n",
            "[AGENT] Final: info@example.com\n",
            "[t3] agent | s5_extract_email |   425 ms | tools=0 | pass=True\n",
            "[GRAPH] Starting...\n",
            "[GRAPH] Completed, tools=0\n",
            "[t3] graph | s5_extract_email |   432 ms | tools=0 | pass=True\n",
            "\n",
            "Saved -> results_20251004_024536/results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 最新のresults.csvを読み込み\n",
        "csv_files = sorted(pathlib.Path(\".\").glob(\"results_*/results.csv\"))\n",
        "if csv_files:\n",
        "    df = pd.read_csv(csv_files[-1])\n",
        "\n",
        "    # 各Runnerの代表的な出力を表示\n",
        "    print(\"=\"*80)\n",
        "    print(\"SAMPLE OUTPUTS (First trial only)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for task in [\"s0_format\", \"s1_reasoning\", \"s3_tool\"]:\n",
        "        print(f\"\\n### Task: {task} ###\\n\")\n",
        "        subset = df[(df[\"trial\"] == 1) & (df[\"task\"] == task)]\n",
        "        for _, row in subset.iterrows():\n",
        "            print(f\"[{row['runner']:>5}] {row['output'][:300]}\")\n",
        "            print(\"-\"*80)\n",
        "else:\n",
        "    print(\"No results CSV found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjbLzkYjdaBM",
        "outputId": "b4583f23-547a-4684-aa52-b5989efe9a44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SAMPLE OUTPUTS (First trial only)\n",
            "================================================================================\n",
            "\n",
            "### Task: s0_format ###\n",
            "\n",
            "[chain] {\"ok\": true}\n",
            "--------------------------------------------------------------------------------\n",
            "[agent] {\"ok\": true}\n",
            "--------------------------------------------------------------------------------\n",
            "[graph] ERROR: recursion limit\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "### Task: s1_reasoning ###\n",
            "\n",
            "[chain] 42\n",
            "--------------------------------------------------------------------------------\n",
            "[agent] 42\n",
            "--------------------------------------------------------------------------------\n",
            "[graph] Final Answer: 42\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "### Task: s3_tool ###\n",
            "\n",
            "[chain] To calculate the expression, we need to follow the order of operations (PEMDAS):  1. Calculate the exponentiation: 5^3 = 125 2. Multiply 231 by 47: 231 * 47 = 10827 3. Add 125 to 10827: 10827 + 125 = \n",
            "--------------------------------------------------------------------------------\n",
            "[agent] 10947\n",
            "--------------------------------------------------------------------------------\n",
            "[graph] Observation: 10982 Final Answer: 10982\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最新のCSVを読み込んで分析\n",
        "import pandas as pd\n",
        "\n",
        "csv_files = sorted(pathlib.Path(\".\").glob(\"results_*/results.csv\"))\n",
        "if csv_files:\n",
        "    df = pd.read_csv(csv_files[-1])\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"SUCCESS RATE BY RUNNER\")\n",
        "    print(\"=\"*80)\n",
        "    summary = df.groupby('runner').agg({\n",
        "        'passed': ['sum', 'count', 'mean']\n",
        "    }).round(3)\n",
        "    print(summary)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUCCESS RATE BY TASK\")\n",
        "    print(\"=\"*80)\n",
        "    summary2 = df.groupby('task').agg({\n",
        "        'passed': ['sum', 'count', 'mean']\n",
        "    }).round(3)\n",
        "    print(summary2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FAILED TASKS - SAMPLE OUTPUTS\")\n",
        "    print(\"=\"*80)\n",
        "    failed = df[(df['passed'] == 0) & (df['trial'] == 1)]\n",
        "    for _, row in failed.head(10).iterrows():\n",
        "        print(f\"\\n[{row['runner']:>5}] {row['task']}\")\n",
        "        print(f\"Output: {row['output'][:150]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frV2T2F9UIRs",
        "outputId": "7e00eda1-dc56-4991-ca2e-570a4b204e01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SUCCESS RATE BY RUNNER\n",
            "================================================================================\n",
            "       passed           \n",
            "          sum count mean\n",
            "runner                  \n",
            "agent      12    15  0.8\n",
            "chain      12    15  0.8\n",
            "graph       9    15  0.6\n",
            "\n",
            "================================================================================\n",
            "SUCCESS RATE BY TASK\n",
            "================================================================================\n",
            "                 passed             \n",
            "                    sum count   mean\n",
            "task                                \n",
            "s0_format             6     9  0.667\n",
            "s1_reasoning          9     9  1.000\n",
            "s3_tool               3     9  0.333\n",
            "s4_context_qa         6     9  0.667\n",
            "s5_extract_email      9     9  1.000\n",
            "\n",
            "================================================================================\n",
            "FAILED TASKS - SAMPLE OUTPUTS\n",
            "================================================================================\n",
            "\n",
            "[graph] s0_format\n",
            "Output: ERROR: recursion limit\n",
            "\n",
            "[chain] s3_tool\n",
            "Output: To calculate the expression, we need to follow the order of operations (PEMDAS):  1. Calculate the exponentiation: 5^3 = 125 2. Multiply 231 by 47: 23\n",
            "\n",
            "[agent] s3_tool\n",
            "Output: 10947\n",
            "\n",
            "[graph] s4_context_qa\n",
            "Output: ERROR: recursion limit\n"
          ]
        }
      ]
    }
  ]
}